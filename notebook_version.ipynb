{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6784kGyFi7jm"
      },
      "source": [
        "# 4chan data collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtPCt4_yg449"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from pathlib import Path\n",
        "import requests\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "import os\n",
        "import logging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4RONQdJhPNF"
      },
      "source": [
        "Helper Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRHpAgXAhE-z"
      },
      "outputs": [],
      "source": [
        "class LoggerManager:\n",
        "    \"\"\"\n",
        "    Logger class for scraper tool\n",
        "    \"\"\"\n",
        "    def __init__(self, base_save_path: Path, logfolderpath: Path, save_log: bool):\n",
        "        self.save_log = save_log\n",
        "        if self.save_log:\n",
        "            self.logfolder = base_save_path / logfolderpath\n",
        "            self.logfolder.mkdir(parents=True, exist_ok=True)\n",
        "        self.logfolder = base_save_path / logfolderpath\n",
        "        self.logger = None\n",
        "\n",
        "    def setup_logging(self, stream_log_level=logging.INFO):\n",
        "        \"\"\"\n",
        "        Setup logger\n",
        "        \"\"\"\n",
        "        self.logger = logging.getLogger(\"4chan_requester\")\n",
        "        self.logger.setLevel(logging.DEBUG)\n",
        "        log_formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s: %(threadName)s - %(message)s\")\n",
        "\n",
        "        streamlogs = logging.StreamHandler()\n",
        "        streamlogs.setLevel(stream_log_level)\n",
        "        streamlogs.setFormatter(log_formatter)\n",
        "        self.logger.addHandler(streamlogs)\n",
        "\n",
        "        if self.save_log:\n",
        "            self._setup_save_logging(log_formatter)\n",
        "\n",
        "        self.logger.debug(\"Logger Initialized\")\n",
        "\n",
        "    def _setup_save_logging(self, log_formatter):\n",
        "        infologpath = self.logfolder / (\"info_log\" + self._get_full_time() + \".log\")\n",
        "        infologfile = logging.FileHandler(infologpath)\n",
        "        infologfile.setLevel(logging.INFO)\n",
        "        infologfile.setFormatter(log_formatter)\n",
        "        self.logger.addHandler(infologfile)\n",
        "\n",
        "        debuglogpath = self.logfolder / (\"debug_log\" + self._get_full_time() + \".log\")\n",
        "        debuglogfile = logging.FileHandler(debuglogpath)\n",
        "        debuglogfile.setLevel(logging.DEBUG)\n",
        "        debuglogfile.setFormatter(log_formatter)\n",
        "        self.logger.addHandler(debuglogfile)\n",
        "\n",
        "    def _get_full_time(self):\n",
        "        now = datetime.utcnow()\n",
        "        return now.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
        "\n",
        "    def cleanup_old_logs(self, days_to_keep: int = 3):\n",
        "        \"\"\"\n",
        "        Clean up/Delete old logs\n",
        "        \"\"\"\n",
        "        now = datetime.utcnow()\n",
        "        threshold_date = now - timedelta(days=days_to_keep)\n",
        "\n",
        "        for log_file in self.logfolder.glob(\"*.log\"):\n",
        "            file_date_str = log_file.name[-len(\"yyyy_mm_dd_hh_mm_ss.log\"):-len(\".log\")]\n",
        "            file_date = datetime.strptime(file_date_str, \"%Y_%m_%d_%H_%M_%S\")\n",
        "\n",
        "            if file_date < threshold_date:\n",
        "                os.remove(log_file)\n",
        "\n",
        "    def get_logger(self):\n",
        "        \"\"\"\n",
        "        :return: the logger object\n",
        "        \"\"\"\n",
        "        if self.logger is None:\n",
        "            raise RuntimeError(\"Logger not set up yet. Call setup_logging() first.\")\n",
        "        return self.logger\n",
        "\n",
        "def get_time():\n",
        "    \"\"\"\n",
        "    :return: time information\n",
        "    \"\"\"\n",
        "    now = datetime.utcnow()\n",
        "    return now.strftime(\"_%H_%M_%S\")\n",
        "\n",
        "def get_day():\n",
        "    \"\"\"\n",
        "    :return: day time information\n",
        "    \"\"\"\n",
        "    now = datetime.utcnow()\n",
        "    return now.strftime(\"%Y_%m_%d\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbOufp-BhZNP"
      },
      "source": [
        "Board Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Spo5zOEhNOH"
      },
      "outputs": [],
      "source": [
        "class Board:\n",
        "    \"\"\"\n",
        "    This is a board object that handles the request, saving, tracking to a particular board\n",
        "    \"\"\"\n",
        "    def __init__(self, board_code, logger):\n",
        "\n",
        "        # Board Code\n",
        "        self.board_code = board_code\n",
        "\n",
        "        # Logger\n",
        "        self.logger = logger\n",
        "\n",
        "        # API address\n",
        "        self.thread_list_api = f'https://a.4cdn.org/{self.board_code}/threads.json' #TODO import from static definitions\n",
        "        self.thread_content_api_prefix = f'https://a.4cdn.org/{self.board_code}/thread' #TODO +/op_id +.json import from static definitions\n",
        "\n",
        "        # Saving paths\n",
        "        self.base_save_path = Path().resolve() / \"data\"\n",
        "        self.update_saving_folder_info()\n",
        "\n",
        "        # For data request interval\n",
        "        self.thread_list_last_request = None\n",
        "        self.thread_content_last_request = {} # Its only useful for adding headers for thread content API\n",
        "        self.thread_list_request_interval = 10 #TODO Static?\n",
        "        self.thread_content_request_interval = 1 #TODO Static?\n",
        "\n",
        "        # Not sure\n",
        "        self.tracking_threads = {} # threads that are observed for their status including last update and death\n",
        "        self.online_threads = []\n",
        "\n",
        "        # Load Saved Files\n",
        "        self.get_previously_saved_info()\n",
        "\n",
        "    def update_saving_folder_info(self): #TODO, need to match the new logic where timestamp does not result duplicate file\n",
        "        \"\"\"\n",
        "        Set up all the saving path in the class, this method is called repeatedly because save path is related to current timestamp\n",
        "        \"\"\"\n",
        "        timestamp = get_day()\n",
        "        # #TODO, static and argument accept?\n",
        "        self.thread_list_path = self.base_save_path / \"saves\" / timestamp / \"threads_on_boards\"\n",
        "        self.thread_content_path = self.base_save_path / \"saves\" / timestamp / \"threads\" / self.board_code\n",
        "        self.thread_list_path.mkdir(parents=True, exist_ok=True)\n",
        "        self.thread_content_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def get_previously_saved_info(self):\n",
        "        \"\"\"\n",
        "        Load previously saved data into the class to prevent downloading the same thread and help monitoring update\n",
        "        \"\"\"\n",
        "        self.logger.debug(\"Checking for past captures of old threads in previous instances\")\n",
        "\n",
        "        for prev_thread_list_path in self.thread_list_path.iterdir(): # get file names inside the folder\n",
        "            if prev_thread_list_path.name.split(\"_\")[0] == str(self.board_code):\n",
        "                prev_thread_counts = 0\n",
        "                with open(prev_thread_list_path, \"r\") as prev_thread_list_file:\n",
        "                    prev_threads = json.load(prev_thread_list_file)\n",
        "                    for page in prev_threads:\n",
        "                        for threads in page[\"threads\"]:\n",
        "                            self.tracking_threads[str(threads[\"no\"])] = [int(threads[\"last_modified\"]), int(threads[\"replies\"])]\n",
        "                            prev_thread_counts += 1\n",
        "                self.logger.debug(f\"{prev_thread_counts} past captures of old threads in previous instances of {self.board_code} discovered\")\n",
        "                # old_monitor_dict will lookg like old_monitor_dict['po']['thread_no'] = [last modified, reply counts]\n",
        "                self.logger.debug(\n",
        "                    f\"{prev_thread_counts} past captures of old threads in previous instances discovered\"\n",
        "                )\n",
        "                return\n",
        "\n",
        "        self.logger.info(f\"No previous thread information for /{self.board_code}/, no old threads to monitor\")\n",
        "\n",
        "    def get_online_thread_list(self):\n",
        "        \"\"\"Request the list of thread IDs on the board\n",
        "        :return: thread id list\n",
        "        \"\"\"\n",
        "        #TODO this can probably be a class too because it can be written in the same way like get_thread_content\n",
        "        self.logger.debug(f\"Board /{self.board_code}/ thread information requested\")\n",
        "        if self.thread_list_last_request == None:\n",
        "            request_response = requests.get(self.thread_list_api)\n",
        "        else:\n",
        "            thread_list_request_interval = datetime.now() - datetime.fromtimestamp(time.mktime(self.thread_list_last_request))\n",
        "            if thread_list_request_interval < timedelta(seconds=10):\n",
        "                sleeping = 10 - thread_list_request_interval.total_seconds()\n",
        "                self.logger.info(f\"Sleeping for {sleeping} seconds: time between requests for threads on board {self.board_code} too short\")\n",
        "                time.sleep(sleeping)\n",
        "\n",
        "            last_modified_time_header = self._format_time_header(self.thread_list_last_request)\n",
        "            request_response = requests.get(self.thread_list_api, headers=last_modified_time_header)\n",
        "\n",
        "        if request_response.status_code == 200:\n",
        "            self.thread_list_last_request = datetime.now().timetuple()\n",
        "            return request_response.json()\n",
        "\n",
        "        if request_response.status_code == 304:\n",
        "            self.logger.info(f\"No new threads on board /{self.board_code}/\")\n",
        "            return None\n",
        "\n",
        "        if request_response.status_code == 404: #TODO should probably implement retry mechanism, if the website is down our program is also down which is not good\n",
        "            self.logger.info(f\"Error when trying to fetch /{self.board_code}/\")\n",
        "            raise Exception(f\"404 when trying to fetch /{self.board_code}/\")\n",
        "\n",
        "    def save_thread_list(self, thread_list):\n",
        "        \"\"\"Save the thread ID list in local directory\n",
        "        \"\"\"\n",
        "        self.update_saving_folder_info()\n",
        "\n",
        "        #TODO, need to match the new logic where timestamp does not result duplicate file\n",
        "        #TODO, static and argument accept?\n",
        "        filename = self.board_code + get_time() + \".json\"\n",
        "\n",
        "        # delete old thread list\n",
        "        for prev_thread_list_path in self.thread_list_path.iterdir():\n",
        "            if prev_thread_list_path.name.split(\"_\")[0] == str(self.board_code):\n",
        "                prev_thread_list_path.unlink()\n",
        "\n",
        "        with open(self.thread_list_path / filename, \"w\") as outfile:\n",
        "            json.dump(thread_list, outfile, indent=2)\n",
        "\n",
        "    def get_thread_content(self, thread_id):\n",
        "        \"\"\"Based on given thread ID, request the content of the thread\n",
        "\n",
        "        :return: thread content\n",
        "        \"\"\"\n",
        "        thread_api_address = self.thread_content_api_prefix + \"/\" + str(thread_id) + \".json\"\n",
        "\n",
        "        request_attempt = 0\n",
        "        request_response = None\n",
        "\n",
        "        while self._check_retry(request_response, thread_id, request_attempt) == True:\n",
        "            if thread_id not in self.thread_content_last_request:\n",
        "                # download thread content without header\n",
        "                request_response = requests.get(thread_api_address)\n",
        "                # record download time to thread_content_last_request if request is successful\n",
        "                if request_response.status_code in [200, 304]:\n",
        "                    self.thread_content_last_request[thread_id] = datetime.now().timetuple()\n",
        "            else:\n",
        "                # get last request time for this thread\n",
        "                last_modified_time_header = self._format_time_header(self.thread_content_last_request[thread_id])\n",
        "                # download thread content with header\n",
        "                request_response = requests.get(thread_api_address, headers=last_modified_time_header)\n",
        "                # record download time to thread_content_last_request if request is successful\n",
        "                if request_response.status_code in [200, 304]:\n",
        "                    self.thread_content_last_request[thread_id] = datetime.now().timetuple()\n",
        "\n",
        "            request_attempt += 1\n",
        "        time.sleep(self.thread_content_request_interval)\n",
        "        return request_response.json() if request_response is not None else None\n",
        "\n",
        "    def save_thread_content(self, thread_id, thread_content):\n",
        "        \"\"\"Save the thread content in local directory\n",
        "        \"\"\"\n",
        "        if thread_content is None:\n",
        "            self.logger.warning(f\"Can't save board {self.board_code}, post {thread_id}, likely 404 during requesting, skip saving\")\n",
        "            return\n",
        "\n",
        "        self.update_saving_folder_info()\n",
        "        #TODO, need to match the new logic where timestamp does not result duplicate file\n",
        "        #TODO, static and argument accept?\n",
        "        filename = str(thread_id) + get_time() + \".json\"\n",
        "        fullname = self.thread_content_path / filename\n",
        "\n",
        "        for saved_thread_path in self.thread_content_path.iterdir():\n",
        "            # check if the thread id is found in saved content, then delete it and save a new one\n",
        "            if int(saved_thread_path.name.split(\"_\")[0]) == int(thread_id):\n",
        "                #TODO this insure there is only one copy for one post, so date naming system needs to be changed\n",
        "                #TODO as well as moving dead threads to an fully saved folder\n",
        "                #TODO name the still tracking, saving folder as something like tracking saved\n",
        "                os.remove(saved_thread_path)\n",
        "\n",
        "        with open(fullname, \"w\") as outfile:\n",
        "            json.dump(thread_content, outfile, indent=2)\n",
        "\n",
        "    def get_threads_to_update(self, online_threads):\n",
        "        \"\"\"Comapre the currently tracking thread and the thread online, see if there are thread die out or require update\n",
        "        :return: thread list require update (download)\n",
        "        \"\"\"\n",
        "        threads_to_update = []\n",
        "\n",
        "        death_count = 0 # previously saved thread it does not match the ones on the current extract threadlist, meaning it's long gone and dead\n",
        "        birth_count = 0 # added new board's each thread or thread id does not exist in the previously saved id (monitoring_threads), but id exist in the currently extracted threadlist, meaning its a new thread\n",
        "        update_count = 0 # when thread in threadlist matches id on monitoring_threads, and its last update is newer, then update it\n",
        "\n",
        "        online_threads = self._process_online_threads(online_threads) # basically the current threads on board\n",
        "\n",
        "        # Check if any tracking thread is dead(disappeared) online\n",
        "        dead_thread_ids = []\n",
        "        for thread_id in self.tracking_threads:\n",
        "            if thread_id not in online_threads:\n",
        "                self.logger.debug(f\"Thread died: /{self.board_code}/{thread_id}\")\n",
        "                death_count += 1\n",
        "                dead_thread_ids.append(thread_id)\n",
        "\n",
        "        # Remove dead threads from tracking list and request history\n",
        "        for dead_thread_id in dead_thread_ids:\n",
        "            del self.tracking_threads[dead_thread_id]\n",
        "            if dead_thread_id in self.thread_content_last_request:\n",
        "                del self.thread_content_last_request[dead_thread_id]\n",
        "\n",
        "        for thread_id in online_threads:\n",
        "            if thread_id in self.tracking_threads:\n",
        "                # online thread is already being tracked, update if needed\n",
        "                tracked_last_modified_time, _ = self.tracking_threads[thread_id]\n",
        "                online_last_modified_time, _ = self.tracking_threads[thread_id]\n",
        "                if (tracked_last_modified_time < online_last_modified_time):\n",
        "                    self.logger.debug(f\"Thread updated: /{self.board_code}/{thread_id}\")\n",
        "                    self.tracking_threads[thread_id] = online_threads[thread_id]\n",
        "                    threads_to_update.append(thread_id)  # posts to update records the board and thread we need to download the content of\n",
        "                    update_count += 1\n",
        "                else:\n",
        "                    self.logger.debug(f\"Do not need to update thread /{self.board_code}/{thread_id}\")\n",
        "            else:\n",
        "                # online thread is not tracked, it's a new thread\n",
        "                self.logger.debug(f\"New thread: /{self.board_code}/{thread_id}\")\n",
        "                self.tracking_threads[thread_id] = online_threads[thread_id]\n",
        "                threads_to_update.append(thread_id)\n",
        "                birth_count += 1\n",
        "\n",
        "        self.logger.info(f\"Thread deaths in previous iteration: {death_count}\")\n",
        "        self.logger.info(f\"Thread births in previous iteration: {birth_count}\")\n",
        "        self.logger.info(f\"Thread updates in previous iteration: {update_count}\")\n",
        "        self.logger.info(f\"{len(self.tracking_threads)} threads are currently being monitored.\")\n",
        "\n",
        "        return threads_to_update\n",
        "\n",
        "    def _process_online_threads(self, online_threads):\n",
        "        proccessed_threads = {}\n",
        "        for page in online_threads:\n",
        "            for thread in page[\"threads\"]:\n",
        "                proccessed_threads[str(thread[\"no\"])] = [int(thread[\"last_modified\"]), int(thread[\"replies\"])]\n",
        "        return proccessed_threads\n",
        "\n",
        "    def _check_retry(self, request_response, thread_id, attempt):\n",
        "        # TODO can change this to strategy mapping, having a factory class that generate strategy mapping for threadlist and thread, and a dict to map to return strategy and log info\n",
        "        if request_response == None:\n",
        "            return True\n",
        "\n",
        "        if request_response.status_code == 304: # if the content does not change since last request, return none\n",
        "            self.logger.debug(f\"Thread {thread_id} not updated since last request\")\n",
        "            return False\n",
        "\n",
        "        if request_response.status_code == 200:\n",
        "            self.logger.debug(\"Recieved answer\")\n",
        "            return False\n",
        "\n",
        "        error_message = f\"Request for thread {thread_id} on board /{self.board_code}/ was unsuccessful with error code {request_response.status_code}.\"\n",
        "        if request_response.status_code == 404:\n",
        "            self.logger.warning(f'{error_message} Skipping')\n",
        "            return False\n",
        "\n",
        "        if attempt <= 5:\n",
        "            self.logger.error(f'{error_message} Current Attempt: {attempt}, now entering next attempt')\n",
        "            time.sleep(self.thread_content_request_interval * 5)\n",
        "            return True\n",
        "        else:\n",
        "            self.logger.warning(f'{error_message} Returning None')\n",
        "            return False\n",
        "\n",
        "    def _format_time_header(self, since):\n",
        "        since = time.gmtime(time.mktime(since))\n",
        "        return {\"If-Modified-Since\": time.strftime(\"%a, %d %b %Y %H:%M:%S GMT\", since)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3epJa01ShcYy"
      },
      "source": [
        "Request Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QS4AqOGghH5d"
      },
      "outputs": [],
      "source": [
        "class Requester:\n",
        "    \"\"\"\n",
        "    The main class for 4chan scraper, it handles creation of Board and Logger class, triggering each methods in Board class for entire scrapping process\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        boards: list,\n",
        "        exclude_boards: bool = False,\n",
        "        request_time_limit: float = 1,\n",
        "        log_folder_path: str = \"logs\",\n",
        "        save_log: bool = True,\n",
        "        clean_log: bool = True\n",
        "    ):\n",
        "        self._base_save_path: Path = Path().resolve() / \"data\" # .resolve() creates absolute path\n",
        "\n",
        "        # Setup Logger\n",
        "        self._log_manager = LoggerManager(self._base_save_path, log_folder_path, save_log)\n",
        "        self._log_manager.setup_logging(stream_log_level=logging.INFO)\n",
        "        self.logger = self._log_manager.get_logger()\n",
        "        self._clean_log = clean_log\n",
        "\n",
        "        # Setup request time interval variables\n",
        "        self._last_request = None\n",
        "        self._request_time_limit: float = request_time_limit\n",
        "\n",
        "        # Setup monitoring boards\n",
        "        self._include_boards: list = boards\n",
        "        self._exclude_boards: bool = exclude_boards\n",
        "        self._monitoring_boards = self._set_monitoring_boards()\n",
        "\n",
        "        # Start scraping pipeline\n",
        "        self._begin_monitoring()\n",
        "\n",
        "    def _begin_monitoring(self):\n",
        "        \"\"\"\n",
        "        Start the monitoring program.\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Beginning monitoring\")\n",
        "        self.logger.info(f\"Storing data in path: {self._base_save_path}\")\n",
        "        self.logger.debug(\"Monitoring Started\")\n",
        "        self._run_scraping_pipeline()\n",
        "\n",
        "    def _run_scraping_pipeline(self):\n",
        "        self.logger.debug(\"scraping_pipeline_monitoring entered\")\n",
        "\n",
        "        # Data Collection Loop\n",
        "        while True:\n",
        "            self.logger.debug(\"Started loop\")\n",
        "            #TODO add check new board mechanism?\n",
        "            for board in self._monitoring_boards:\n",
        "\n",
        "                self._check_time_and_wait()\n",
        "                online_thread_list = board.get_online_thread_list()\n",
        "\n",
        "                if online_thread_list is not None: #TODO this is weird. where should the update checking logic be used?\n",
        "                    board.save_thread_list(online_thread_list)\n",
        "                    threads_to_update = board.get_threads_to_update(online_thread_list)\n",
        "\n",
        "                    self.logger.info(f\"Updating posts in {board.board_code}\")\n",
        "                    n_threads_to_update = len(threads_to_update)\n",
        "                    i = 1\n",
        "                    for thread_id in threads_to_update: #TODO incorporate getting thread content\"s\" without looping here?\n",
        "                        start_time = time.time()\n",
        "\n",
        "                        self._check_time_and_wait()\n",
        "                        thread_content = board.get_thread_content(thread_id)\n",
        "                        board.save_thread_content(thread_id, thread_content)\n",
        "\n",
        "                        current_time_diff = (time.time() - start_time) * (n_threads_to_update - i)\n",
        "                        self.logger.debug(f\"{i}/{n_threads_to_update}: Capturing post {thread_id} in /{board.board_code}/ approximate seconds remaining for this board {current_time_diff:n}\")\n",
        "                        i += 1\n",
        "                self.logger.debug(f\"Ended /{board.board_code}/ collection\")\n",
        "            self.logger.debug(\"Ended loop\")\n",
        "\n",
        "            if self._clean_log:\n",
        "                self.logger.debug(\"Cleaning Log\")\n",
        "                self._log_manager.cleanup_old_logs(days_to_keep=3)\n",
        "\n",
        "    def _set_monitoring_boards(self):\n",
        "        \"\"\"\n",
        "        Preparing self.monitoring_boards which is essentially a list of board code that the program should monitor\n",
        "        Process includes checking self._include_boards and self._exclude_boards\n",
        "        if exclude_boards then self._include_boards become the ones not to monitor\n",
        "        \"\"\"\n",
        "        available_boards = self._get_4chan_board_list()\n",
        "\n",
        "        self.logger.debug(\"Updating monitor board list (checking)\")\n",
        "        if self._include_boards is not None and not self._exclude_boards:\n",
        "            board_list = self._include_boards\n",
        "        elif self._include_boards is not None and self._exclude_boards:\n",
        "            board_list = list(\n",
        "                set(available_boards).difference(self._include_boards)\n",
        "            )\n",
        "        else: # if no boards provided then all boards are monitored\n",
        "            board_list = available_boards\n",
        "        # check if all the boards that will be monitored are valid\n",
        "        for board in board_list:\n",
        "            if board not in available_boards:\n",
        "                self.logger.info(f\"Board code '{board}' is not available in 4chan\")\n",
        "                raise KeyError(f\"Board code '{board}' is not available in 4chan\")\n",
        "\n",
        "        # initialize Board class list\n",
        "        monitoring_boards = []\n",
        "        for board in board_list:\n",
        "            monitoring_boards.append(Board(board, self.logger))\n",
        "        self.logger.debug(\"Old monitors retrieved\")\n",
        "        #TODO Caculate overall pre_threads after all boards are initialized\n",
        "        #self.logger.debug(f\"{old_threads} past captures of old threads in previous instances discovered\")\n",
        "\n",
        "        return monitoring_boards\n",
        "\n",
        "    def _get_4chan_board_list(self):\n",
        "        self._check_time_and_wait()\n",
        "        self.logger.debug(\"chan information requested\")\n",
        "        boards = requests.get(\"http://a.4cdn.org/boards.json\")\n",
        "        boards_info = boards.json()\n",
        "        codes = [board[\"board\"] for board in boards_info[\"boards\"]]\n",
        "        return codes\n",
        "\n",
        "    def _check_time_and_wait(self):\n",
        "        if self._last_request is None:\n",
        "            self._last_request = time.time()\n",
        "        else:\n",
        "            cur_time = time.time()\n",
        "            if cur_time - self._last_request >= self._request_time_limit:\n",
        "                self._last_request = cur_time\n",
        "            else:\n",
        "                time.sleep(self._request_time_limit - (cur_time - self._last_request))\n",
        "                self._last_request = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0iO1zFNhpCS"
      },
      "source": [
        "Run Requester"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JbbG8z_ZhqT_",
        "outputId": "6b94f8d5-e0b3-4bd9-aa4b-a2d137733400"
      },
      "outputs": [],
      "source": [
        "requester_instance = Requester(\n",
        "            boards=['adv', 'toy'],\n",
        "            exclude_boards=False,\n",
        "            request_time_limit=1,\n",
        "            log_folder_path='logs',\n",
        "            save_log=False,\n",
        "            clean_log=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
